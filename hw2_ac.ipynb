{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPxcHTEgjl3/lKQdLaBFnH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoshithavalluru/assignment-2/blob/main/hw2_ac.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmGrtM8YZzyd"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium==0.27.1 -q\n",
        "!pip install gymnasium[box2d] -q\n",
        "!pip install moviepy -q\n",
        "!pip install -U kora -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def apply_discount(raw_reward, gamma=0.99):\n",
        "    raw_reward.reverse()\n",
        "    sum = 0\n",
        "    discounted_rtg_reward = []\n",
        "    for reward in raw_reward:\n",
        "        sum = sum*gamma+reward\n",
        "        discounted_rtg_reward.append(sum)\n",
        "    raw_reward.reverse()\n",
        "    discounted_rtg_reward.reverse()\n",
        "    # Normalization\n",
        "    discounted_rtg_reward = np.array(discounted_rtg_reward)\n",
        "    discounted_rtg_reward = discounted_rtg_reward - np.mean(discounted_rtg_reward) / (np.std(discounted_rtg_reward) + np.finfo(np.float32).eps)\n",
        "    return torch.tensor(discounted_rtg_reward, dtype=torch.float32, device=get_device())\n",
        "\n",
        "    \n",
        "# Util function to apply reward-return (cumulative reward) on a list of instant-reward (from eq 6)\n",
        "def apply_return(raw_reward):\n",
        "    # Compute r_reward (as a list) from raw_reward\n",
        "    r_reward = [np.sum(raw_reward)]\n",
        "    return torch.tensor(r_reward, dtype=torch.float32, device=get_device())  \n",
        "\n",
        "def pad_arrays(arr_list):\n",
        "    \n",
        "    if not all(isinstance(arr, np.ndarray) for arr in arr_list):\n",
        "        raise ValueError(\"arr_list must contain only numpy arrays\")\n",
        "    max_shape = np.max([arr.shape for arr in arr_list], axis=0)\n",
        "    padded_arr_list = []\n",
        "    for arr in arr_list:\n",
        "        pad_width = [(0, max_shape[i] - arr.shape[i]) for i in range(len(max_shape))]\n",
        "        padded_arr = np.pad(arr, pad_width, mode='constant')\n",
        "        padded_arr_list.append(padded_arr)\n",
        "    return padded_arr_list "
      ],
      "metadata": {
        "id": "gpG2falEZ3g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/MyDrive/Colab Notebooks/videos\n",
        "!ls"
      ],
      "metadata": {
        "id": "dBKyfjciZ7zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import pickle\n",
        "import random\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "from collections import deque, namedtuple\n",
        "from gymnasium.utils.save_video import save_video\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.distributions import Categorical\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# Class for training an RL agent with Actor-Critic\n",
        "class ACTrainer:\n",
        "    def __init__(self, params):\n",
        "        self.params = params\n",
        "        self.env = gym.make(self.params['env_name'])\n",
        "        self.agent = ACAgent(env=self.env, params=self.params)\n",
        "        self.actor_net = ActorNet(input_size=self.env.observation_space.shape[0], output_size=self.env.action_space.n, hidden_dim=self.params['hidden_dim']).to(get_device())\n",
        "        self.critic_net = CriticNet(input_size=self.env.observation_space.shape[0], output_size=1, hidden_dim=self.params['hidden_dim']).to(get_device())\n",
        "        self.actor_optimizer = Adam(params=self.actor_net.parameters(), lr=self.params['actor_lr'])\n",
        "        self.critic_optimizer = Adam(params=self.critic_net.parameters(), lr=self.params['critic_lr'])\n",
        "        self.trajectory = None\n",
        "\n",
        "    def run_training_loop(self):\n",
        "        list_ro_reward = list()\n",
        "        for ro_idx in range(self.params['n_rollout']):\n",
        "            self.trajectory = self.agent.collect_trajectory(policy=self.actor_net)\n",
        "            self.update_critic_net()\n",
        "            self.estimate_advantage()\n",
        "            self.update_actor_net()\n",
        "            sum_of_rewards = 0\n",
        "            reward_list = self.trajectory.get('reward')\n",
        "            for trajectory_reward_list in reward_list:\n",
        "                sum_of_rewards += apply_return(trajectory_reward_list)\n",
        "            avg_ro_reward = (sum_of_rewards/len(reward_list)).item()\n",
        "            print(f'End of rollout {ro_idx}: Average trajectory reward is {avg_ro_reward: 0.2f}')\n",
        "            # Append average rollout reward into a list\n",
        "            list_ro_reward.append(avg_ro_reward)\n",
        "        # Save avg-rewards as pickle files\n",
        "        pkl_file_name = self.params['exp_name'] + '.pkl'\n",
        "        with open(pkl_file_name, 'wb') as f:\n",
        "            pickle.dump(list_ro_reward, f)\n",
        "        # Save a video of the trained agent playing\n",
        "        self.generate_video()\n",
        "        # Close environment\n",
        "        self.env.close()\n",
        "\n",
        "    def update_critic_net(self):\n",
        "        for critic_iter_idx in range(self.params['n_critic_iter']):\n",
        "            self.update_target_value()\n",
        "            for critic_epoch_idx in range(self.params['n_critic_epoch']):\n",
        "                critic_loss = self.estimate_critic_loss_function()\n",
        "                critic_loss.backward()\n",
        "                self.critic_optimizer.step()\n",
        "                self.critic_optimizer.zero_grad()\n",
        "\n",
        "    def update_target_value(self, gamma=0.99):\n",
        "        self.trajectory['state_value'] = []\n",
        "        self.trajectory['target_value'] = []\n",
        "        for i in range(len(self.trajectory['obs'])):\n",
        "            # Calculate state value for current time step\n",
        "            state_value_estimate = self.critic_net(self.trajectory['obs'][i])\n",
        "            self.trajectory['state_value'].append(state_value_estimate.detach().numpy())\n",
        "            target_value = self.trajectory['reward'][i]\n",
        "            if i < len(self.trajectory['obs']) - 1:\n",
        "                target_value= gamma * self.critic_net(self.trajectory['obs'][i+1]).detach().numpy()\n",
        "            if isinstance(target_value, list):\n",
        "                target_value = np.array(target_value)    \n",
        "            print(\"type is \",type(target_value))\n",
        "            self.trajectory['target_value'].append(target_value);\n",
        "\n",
        "\n",
        "\n",
        "    def estimate_advantage(self, gamma=0.99): \n",
        "       state_value = self.trajectory['state_value'];\n",
        "       target_value = self.trajectory['target_value'];\n",
        "       max_dim = max(arr.ndim for arr in state_value if isinstance(arr, np.ndarray))\n",
        "       for i in range(len(target_value)):\n",
        "           if isinstance(target_value[i], np.ndarray):\n",
        "               while target_value[i].ndim < max_dim:\n",
        "                   target_value[i] = np.expand_dims(target_value[i], axis=0)\n",
        "               while target_value[i].ndim > max_dim:\n",
        "                   target_value[i] = np.squeeze(target_value[i], axis=0)\n",
        "       # Make sure all the arrays in target_value have the same shape along axis 1 as the arrays in state_value\n",
        "       target_value = [arr[:, :state_value[i].shape[1]] if isinstance(arr, np.ndarray) else arr for i, arr in enumerate(target_value)]\n",
        "                \n",
        "       tensor1 = torch.tensor(np.concatenate(state_value, axis=0))\n",
        "       tensor2 = torch.tensor(np.concatenate(target_value, axis=0))\n",
        "       result = tensor2 - tensor1\n",
        "       self.trajectory['advantage'] = result.detach().numpy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def update_actor_net(self):\n",
        "        actor_loss = self.estimate_actor_loss_function()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "\n",
        "    def estimate_critic_loss_function(self):\n",
        "        critic_loss = list()\n",
        "        for t_idx in range(self.params['n_trajectory_per_rollout']):\n",
        "            R_t = apply_discount(self.trajectory['reward'][t_idx])\n",
        "            V_s = self.critic_net(self.trajectory['obs'][t_idx]).squeeze()\n",
        "            loss = F.mse_loss(V_s, R_t)\n",
        "            critic_loss.append(loss)\n",
        "        critic_loss = torch.stack(critic_loss).mean()\n",
        "        return critic_loss\n",
        "\n",
        "\n",
        "    def estimate_actor_loss_function(self):\n",
        "        actor_loss = list()\n",
        "        for t_idx in range(self.params['n_trajectory_per_rollout']):\n",
        "            advantage = apply_discount(self.trajectory['advantage'][t_idx])\n",
        "            _, log_prob = self.actor_net(self.trajectory['obs'][t_idx])\n",
        "            loss = -torch.mean(advantage * log_prob)\n",
        "            actor_loss.append(loss)\n",
        "        actor_loss = torch.stack(actor_loss).mean()\n",
        "        return actor_loss\n",
        "\n",
        "\n",
        "    def generate_video(self, max_frame=1000):\n",
        "        self.env = gym.make(self.params['env_name'], render_mode='rgb_array_list')\n",
        "        obs, _ = self.env.reset()\n",
        "        for _ in range(max_frame):\n",
        "            action_idx, log_prob = self.actor_net(torch.tensor(obs, dtype=torch.float32, device=get_device()))\n",
        "            obs, reward, terminated, truncated, info = self.env.step(self.agent.action_space[action_idx.item()])\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "        save_video(frames=self.env.render(), video_folder=self.params['env_name'][:-3], fps=self.env.metadata['render_fps'], step_starting_index=0, episode_index=0)\n",
        "\n",
        "\n",
        "\n",
        "class ActorNet(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim):\n",
        "        super(ActorNet, self).__init__()\n",
        "        self.ff_net = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_size),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, obs):\n",
        "        action_probs = self.ff_net(obs)\n",
        "        dist_obj = Categorical(action_probs)\n",
        "        action_index = dist_obj.sample()\n",
        "        log_prob = dist_obj.log_prob(action_index)\n",
        "        return action_index, log_prob\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CriticNet(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim):\n",
        "        super(CriticNet, self).__init__()\n",
        "        self.ff_net = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, obs):\n",
        "        state_value = self.ff_net(obs)\n",
        "        return state_value\n",
        "\n",
        "\n",
        "\n",
        "# Class for agent\n",
        "class ACAgent:\n",
        "    def __init__(self, env, params=None):\n",
        "        self.env = env\n",
        "        self.params = params\n",
        "        self.action_space = [action for action in range(self.env.action_space.n)]\n",
        "\n",
        "    def collect_trajectory(self, policy):\n",
        "        obs, _ = self.env.reset(seed=self.params['rng_seed'])\n",
        "        rollout_buffer = list()\n",
        "        for _ in range(self.params['n_trajectory_per_rollout']):\n",
        "            trajectory_buffer = {'obs': list(), 'log_prob': list(), 'reward': list()}\n",
        "            while True:\n",
        "                obs = torch.tensor(obs, dtype=torch.float32, device=get_device())\n",
        "                # Save observation\n",
        "                trajectory_buffer['obs'].append(obs)\n",
        "                action_idx, log_prob = policy(obs)\n",
        "                obs, reward, terminated, truncated, info = self.env.step(self.action_space[action_idx.item()])\n",
        "                # Save log-prob and reward into the buffer\n",
        "                trajectory_buffer['log_prob'].append(log_prob)\n",
        "                trajectory_buffer['reward'].append(reward)\n",
        "                # Check for termination criteria\n",
        "                if terminated or truncated:\n",
        "                    obs, _ = self.env.reset()\n",
        "                    rollout_buffer.append(trajectory_buffer)\n",
        "                    break\n",
        "        rollout_buffer = self.serialize_trajectory(rollout_buffer)\n",
        "        return rollout_buffer\n",
        "\n",
        "    # Converts a list-of-dictionary into dictionary-of-list\n",
        "    @staticmethod\n",
        "    def serialize_trajectory(rollout_buffer):\n",
        "        serialized_buffer = {'obs': list(), 'log_prob': list(), 'reward': list()}\n",
        "        for trajectory_buffer in rollout_buffer:\n",
        "            serialized_buffer['obs'].append(torch.stack(trajectory_buffer['obs']))\n",
        "            serialized_buffer['log_prob'].append(torch.stack(trajectory_buffer['log_prob']))\n",
        "            serialized_buffer['reward'].append(trajectory_buffer['reward'])\n",
        "        return serialized_buffer"
      ],
      "metadata": {
        "id": "M3X2RL9hZ-nB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set simulation parameters\n",
        "params = {\n",
        "    'env_name': 'LunarLander-v2',\n",
        "    'rng_seed': 6369,\n",
        "    'n_rollout': 100,\n",
        "    'n_trajectory_per_rollout': 60,\n",
        "    'n_critic_iter': 1,\n",
        "    'n_critic_epoch': 1,\n",
        "    'hidden_dim': 128,\n",
        "    'actor_lr': 3e-3,\n",
        "    'critic_lr': 3e-4,\n",
        "    'exp_name': 'LunarLander_v2_t0'\n",
        "}\n",
        "\n",
        "# Seed RNGs\n",
        "seed_everything(params['rng_seed'])\n",
        "\n",
        "# Train agent\n",
        "trainer = ACTrainer(params)\n",
        "trainer.run_training_loop()"
      ],
      "metadata": {
        "id": "gwRS0vpqaGjT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}