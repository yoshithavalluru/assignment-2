{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVhwZkroNHe2IxSXdiv3K8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoshithavalluru/assignment-2/blob/main/hw2_ac.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmGrtM8YZzyd"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium==0.27.1 -q\n",
        "!pip install gymnasium[box2d] -q\n",
        "!pip install moviepy -q\n",
        "!pip install -U kora -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def apply_discount(raw_reward, gamma=0.99):\n",
        "    raw_reward.reverse()\n",
        "    sum = 0\n",
        "    discounted_rtg_reward = []\n",
        "    for reward in raw_reward:\n",
        "        sum = sum*gamma+reward\n",
        "        discounted_rtg_reward.append(sum)\n",
        "    raw_reward.reverse()\n",
        "    discounted_rtg_reward.reverse()\n",
        "    # Normalization\n",
        "    discounted_rtg_reward = np.array(discounted_rtg_reward)\n",
        "    discounted_rtg_reward = discounted_rtg_reward - np.mean(discounted_rtg_reward) / (np.std(discounted_rtg_reward) + np.finfo(np.float32).eps)\n",
        "    return torch.tensor(discounted_rtg_reward, dtype=torch.float32, device=get_device())\n",
        "\n",
        "    \n",
        "# Util function to apply reward-return (cumulative reward) on a list of instant-reward (from eq 6)\n",
        "def apply_return(raw_reward):\n",
        "    # Compute r_reward (as a list) from raw_reward\n",
        "    r_reward = [np.sum(raw_reward)]\n",
        "    return torch.tensor(r_reward, dtype=torch.float32, device=get_device())  "
      ],
      "metadata": {
        "id": "gpG2falEZ3g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/MyDrive/Colab Notebooks/videos\n",
        "!ls"
      ],
      "metadata": {
        "id": "dBKyfjciZ7zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import pickle\n",
        "import random\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "from collections import deque, namedtuple\n",
        "from gymnasium.utils.save_video import save_video\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "\n",
        "# Class for training an RL agent with Actor-Critic\n",
        "class ACTrainer:\n",
        "    def __init__(self, params):\n",
        "        self.params = params\n",
        "        self.env = gym.make(self.params['env_name'])\n",
        "        self.agent = ACAgent(env=self.env, params=self.params)\n",
        "        self.actor_net = ActorNet(input_size=self.env.observation_space.shape[0], output_size=self.env.action_space.n, hidden_dim=self.params['hidden_dim']).to(get_device())\n",
        "        self.critic_net = CriticNet(input_size=self.env.observation_space.shape[0], output_size=1, hidden_dim=self.params['hidden_dim']).to(get_device())\n",
        "        self.actor_optimizer = Adam(params=self.actor_net.parameters(), lr=self.params['actor_lr'])\n",
        "        self.critic_optimizer = Adam(params=self.critic_net.parameters(), lr=self.params['critic_lr'])\n",
        "        self.trajectory = None\n",
        "\n",
        "    def run_training_loop(self):\n",
        "        list_ro_reward = list()\n",
        "        for ro_idx in range(self.params['n_rollout']):\n",
        "            self.trajectory = self.agent.collect_trajectory(policy=self.actor_net)\n",
        "            self.update_critic_net()\n",
        "            self.estimate_advantage()\n",
        "            self.update_actor_net()\n",
        "            sum_of_rewards = 0\n",
        "            reward_list = self.trajectory['reward']\n",
        "            for trajectory_reward_list in reward_list:\\\n",
        "                sum_of_rewards += apply_return(trajectory_reward_list)\n",
        "            avg_ro_reward = (sum_of_rewards/len(reward_list)).item()\n",
        "            print(f'End of rollout {ro_idx}: Average trajectory reward is {avg_ro_reward: 0.2f}')\n",
        "            # Append average rollout reward into a list\n",
        "            list_ro_reward.append(avg_ro_reward)\n",
        "        # Save avg-rewards as pickle files\n",
        "        pkl_file_name = self.params['exp_name'] + '.pkl'\n",
        "        with open(pkl_file_name, 'wb') as f:\n",
        "            pickle.dump(list_ro_reward, f)\n",
        "        # Save a video of the trained agent playing\n",
        "        self.generate_video()\n",
        "        # Close environment\n",
        "        self.env.close()\n",
        "\n",
        "    def update_critic_net(self):\n",
        "        for critic_iter_idx in range(self.params['n_critic_iter']):\n",
        "            self.update_target_value()\n",
        "            for critic_epoch_idx in range(self.params['n_critic_epoch']):\n",
        "                critic_loss = self.estimate_critic_loss_function()\n",
        "                critic_loss.backward()\n",
        "                self.critic_optimizer.step()\n",
        "                self.critic_optimizer.zero_grad()\n",
        "\n",
        "    def update_target_value(self, gamma=0.99):\n",
        "        state_values = []\n",
        "        target_values = []\n",
        "        obs_list =  self.trajectory['obs'];\n",
        "        reward_list = self.trajectory['reward']\n",
        "        state_value = torch.zeros(1, 1)\n",
        "        for i in range(len(obs_list)):\n",
        "            obs = obs_list[i]\n",
        "            reward = torch.tensor(reward_list[i]).unsqueeze(1)\n",
        "            state_value = self.critic_net.forward(obs)\n",
        "            state_values.append(state_value)\n",
        "            \n",
        "        # for i in range(len(reward_list)):\n",
        "        #     reward = torch.tensor(reward_list[i]).unsqueeze(1)\n",
        "        #     target_value  = reward + gamma * state_values[i]\n",
        "        #     target_values.append(target_value)\n",
        "\n",
        "        self.trajectory['state_value'] = state_values\n",
        "        \n",
        "        next_state_value = torch.zeros(1, 1)\n",
        "        for i in reversed(range(len(obs_list))):\n",
        "            reward = torch.tensor(reward_list[i]).unsqueeze(1)\n",
        "            next_state_value = reward + gamma *  state_values[i]\n",
        "            target_values.append(next_state_value)\n",
        "        self.trajectory['target_value'] = target_values    \n",
        "             \n",
        "\n",
        "    \n",
        "\n",
        "    def estimate_advantage(self, gamma=0.99):\n",
        "        # TODO: Estimate advantage\n",
        "        # HINT: Use definition of advantage-estimate from equation 6 of teh assignment PDF\n",
        "         target_values = torch.cat(self.trajectory['target_value'], dim=0)\n",
        "         state_values = torch.cat(self.trajectory['state_value'], dim=0)\n",
        "         self.trajectory['advantage'] = target_values - state_values\n",
        "\n",
        "\n",
        "    def update_actor_net(self):\n",
        "        actor_loss = self.estimate_actor_loss_function()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "\n",
        "    def estimate_critic_loss_function(self):\n",
        "        # TODO: Compute critic loss function\n",
        "         # Use definition of critic-loss from equation 7 of the assignment PDF. It is the MSE between target-values and state-values.\n",
        "        target_values = torch.cat(self.trajectory['target_value'], dim=0)\n",
        "        state_values = torch.cat(self.trajectory['state_value'], dim=0)\n",
        "        mse = torch.mean(torch.square(target_values - state_values))\n",
        "        return mse\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def estimate_actor_loss_function(self):\n",
        "        actor_loss = list()\n",
        "        for t_idx in range(self.params['n_trajectory_per_rollout']):\n",
        "            advantage = apply_discount(self.trajectory['advantage'][t_idx].tolist())\n",
        "            log_probs = self.trajectory['log_prob'][t_idx]\n",
        "            actor_loss_t = torch.mean(-log_probs * advantage)\n",
        "            actor_loss.append(actor_loss_t)\n",
        "        actor_loss_mean = torch.mean(torch.stack(actor_loss))\n",
        "        return actor_loss_mean\n",
        "\n",
        "    def generate_video(self, max_frame=1000):\n",
        "        self.env = gym.make(self.params['env_name'], render_mode='rgb_array_list')\n",
        "        obs, _ = self.env.reset()\n",
        "        for _ in range(max_frame):\n",
        "            action_idx, log_prob = self.actor_net(torch.tensor(obs, dtype=torch.float32, device=get_device()))\n",
        "            obs, reward, terminated, truncated, info = self.env.step(self.agent.action_space[action_idx.item()])\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "        save_video(frames=self.env.render(), video_folder=self.params['env_name'][:-3], fps=self.env.metadata['render_fps'], step_starting_index=0, episode_index=0)\n",
        "\n",
        "\n",
        "class ActorNet(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim):\n",
        "        super(ActorNet, self).__init__()\n",
        "        # Define the actor net\n",
        "        # Use nn.Sequential to set up a 2 layer feedforward neural network\n",
        "        self.ff_net = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_size),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, obs):\n",
        "        # Forward pass of actor net\n",
        "        # Use Categorical from torch.distributions to draw samples and log-prob from model output\n",
        "        action_probs = self.ff_net(obs)\n",
        "        dist = Categorical(action_probs)\n",
        "        action_index = dist.sample()\n",
        "        log_prob = dist.log_prob(action_index)\n",
        "        return action_index, log_prob\n",
        "\n",
        "\n",
        "class CriticNet(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim):\n",
        "        super(CriticNet, self).__init__()\n",
        "        # Define the critic net\n",
        "        # Use nn.Sequential to set up a 2 layer feedforward neural network\n",
        "        self.ff_net = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, obs):\n",
        "        # Forward pass of critic net\n",
        "        # Get state value from the network using the current observation\n",
        "        state_value = self.ff_net(obs)\n",
        "        return state_value\n",
        "\n",
        "\n",
        "# Class for agent\n",
        "class ACAgent:\n",
        "    def __init__(self, env, params=None):\n",
        "        self.env = env\n",
        "        self.params = params\n",
        "        self.action_space = [action for action in range(self.env.action_space.n)]\n",
        "\n",
        "    def collect_trajectory(self, policy):\n",
        "        obs, _ = self.env.reset(seed=self.params['rng_seed'])\n",
        "        rollout_buffer = list()\n",
        "        for _ in range(self.params['n_trajectory_per_rollout']):\n",
        "            trajectory_buffer = {'obs': list(), 'log_prob': list(), 'reward': list()}\n",
        "            while True:\n",
        "                obs = torch.tensor(obs, dtype=torch.float32, device=get_device())\n",
        "                # Save observation\n",
        "                trajectory_buffer['obs'].append(obs)\n",
        "                action_idx, log_prob = policy(obs)\n",
        "                obs, reward, terminated, truncated, info = self.env.step(self.action_space[action_idx.item()])\n",
        "                # Save log-prob and reward into the buffer\n",
        "                trajectory_buffer['log_prob'].append(log_prob)\n",
        "                trajectory_buffer['reward'].append(reward)\n",
        "                # Check for termination criteria\n",
        "                if terminated or truncated:\n",
        "                    obs, _ = self.env.reset()\n",
        "                    rollout_buffer.append(trajectory_buffer)\n",
        "                    break\n",
        "        rollout_buffer = self.serialize_trajectory(rollout_buffer)\n",
        "        return rollout_buffer\n",
        "\n",
        "    # Converts a list-of-dictionary into dictionary-of-list\n",
        "    @staticmethod\n",
        "    def serialize_trajectory(rollout_buffer):\n",
        "        serialized_buffer = {'obs': list(), 'log_prob': list(), 'reward': list()}\n",
        "        for trajectory_buffer in rollout_buffer:\n",
        "            serialized_buffer['obs'].append(torch.stack(trajectory_buffer['obs']))\n",
        "            serialized_buffer['log_prob'].append(torch.stack(trajectory_buffer['log_prob']))\n",
        "            serialized_buffer['reward'].append(trajectory_buffer['reward'])\n",
        "        return serialized_buffer"
      ],
      "metadata": {
        "id": "M3X2RL9hZ-nB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      # set simulation parameters\n",
        "params = {\n",
        "    'env_name': 'LunarLander-v2',\n",
        "    'rng_seed': 6369,\n",
        "    'n_rollout': 100,\n",
        "    'n_trajectory_per_rollout': 60,\n",
        "    'n_critic_iter': 1,\n",
        "    'n_critic_epoch': 1,\n",
        "    'hidden_dim': 128,\n",
        "    'actor_lr': 3e-3,\n",
        "    'critic_lr': 3e-4,\n",
        "    'exp_name': 'LunarLander_v2_t0'\n",
        "}\n",
        "\n",
        "# Seed RNGs\n",
        "seed_everything(params['rng_seed'])\n",
        "\n",
        "# Train agent\n",
        "trainer = ACTrainer(params)\n",
        "trainer.run_training_loop()"
      ],
      "metadata": {
        "id": "gwRS0vpqaGjT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}