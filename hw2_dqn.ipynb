{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOfdgOZzv+gm0/TxKJPes7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoshithavalluru/assignment-2/blob/main/hw2_dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILBQZ_axWQtJ"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium==0.27.1 -q\n",
        "!pip install gymnasium[box2d] -q\n",
        "!pip install moviepy -q\n",
        "!pip install -U kora -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd Colab Notebooks/videos\n",
        "!ls"
      ],
      "metadata": {
        "id": "B76rup8tWYLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def apply_discount(raw_reward, gamma=0.99):\n",
        "    raw_reward.reverse()\n",
        "    sum = 0\n",
        "    discounted_rtg_reward = []\n",
        "    for reward in raw_reward:\n",
        "        sum = sum*gamma+reward\n",
        "        discounted_rtg_reward.append(sum)\n",
        "    raw_reward.reverse()\n",
        "    discounted_rtg_reward.reverse()\n",
        "    # Normalization\n",
        "    discounted_rtg_reward = np.array(discounted_rtg_reward)\n",
        "    discounted_rtg_reward = discounted_rtg_reward - np.mean(discounted_rtg_reward) / (np.std(discounted_rtg_reward) + np.finfo(np.float32).eps)\n",
        "    return torch.tensor(discounted_rtg_reward, dtype=torch.float32, device=get_device())"
      ],
      "metadata": {
        "id": "OH6QxZ_WWa55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "import random\n",
        "import copy\n",
        "import pickle\n",
        "import random\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "from collections import deque, namedtuple\n",
        "from gymnasium.utils.save_video import save_video\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "class DQNTrainer:\n",
        "    def __init__(self, params):\n",
        "        self.params = params\n",
        "        self.env = gym.make(self.params['env_name'])\n",
        "        self.q_net = QNet(input_size=self.env.observation_space.shape[0], output_size=self.env.action_space.n, hidden_dim=self.params['hidden_dim']).to(get_device())\n",
        "        self.target_net = QNet(input_size=self.env.observation_space.shape[0], output_size=self.env.action_space.n, hidden_dim=self.params['hidden_dim']).to(get_device())\n",
        "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
        "        self.epsilon = self.params['init_epsilon']\n",
        "        self.optimizer = Adam(params=self.q_net.parameters(), lr=self.params['lr'])\n",
        "        self.replay_memory = ReplayMemory(capacity=self.params['rm_cap'])\n",
        "\n",
        "    def run_training_loop(self):\n",
        "        list_ep_reward = list()\n",
        "        obs, _ = self.env.reset(seed=self.params['rng_seed'])\n",
        "        for idx_episode in range(self.params['n_episode']):\n",
        "            ep_len = 0\n",
        "            while True:\n",
        "                ep_len += 1\n",
        "                action = self.get_action(obs)\n",
        "                next_obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "                if terminated or truncated:\n",
        "                    self.epsilon = max(self.epsilon*self.params['epsilon_decay'], self.params['min_epsilon'])\n",
        "                    next_obs = None\n",
        "                    self.replay_memory.push(obs, action, reward, next_obs, not (terminated or truncated))\n",
        "                    list_ep_reward.append(ep_len)\n",
        "                    print(f'End of episode {idx_episode} with epsilon = {self.epsilon: 0.2f} and reward = {ep_len}, memory = {len(self.replay_memory.buffer)}')\n",
        "                    obs, _ = self.env.reset()\n",
        "                    break\n",
        "                self.replay_memory.push(obs, action, reward, next_obs, not (terminated or truncated))\n",
        "                obs = copy.deepcopy(next_obs)\n",
        "                self.update_q_net()\n",
        "                self.update_target_net()\n",
        "        # Save avg-rewards as pickle files\n",
        "        pkl_file_name = self.params['exp_name'] + '.pkl'\n",
        "        with open(pkl_file_name, 'wb') as f:\n",
        "            pickle.dump(list_ep_reward, f)\n",
        "        # Save a video of the trained agent playing\n",
        "        self.generate_video()\n",
        "        # Close environment\n",
        "        self.env.close()\n",
        "\n",
        "    def get_action(self, obs):\n",
        "          epsilon = self.epsilon\n",
        "          obs = torch.from_numpy(obs).float()\n",
        "          # Get Q-values for the current observation\n",
        "          q_values = self.q_net(obs)\n",
        "          if np.random.uniform(0,1) < epsilon:\n",
        "              action = self.env.action_space.sample()\n",
        "          else:\n",
        "              action = q_values.argmax(dim=0).item()\n",
        "          return  action\n",
        "\n",
        "\n",
        "    def update_q_net(self):\n",
        "        if len(self.replay_memory.buffer) < self.params['batch_size']:\n",
        "            return\n",
        "    \n",
        "        # sample a batch from replay memory\n",
        "        state, action, reward, next_state, done = self.replay_memory.sample(self.params['batch_size'])\n",
        "      \n",
        "\n",
        "    # Compute predicted state value for current state using the Q-net\n",
        "        predicted_state_value = self.q_net(state).gather(1, action.unsqueeze(1)).squeeze()\n",
        "\n",
        "    # Compute target value using the Q-net and target network\n",
        "        with torch.no_grad():\n",
        "            next_state_value = self.target_net(next_state).max(1)[0]\n",
        "            target_value = reward + self.params['gamma'] * (1 - done) * next_state_value\n",
        "\n",
        "    # Compute the Q-loss and backpropagate\n",
        "        criterion = nn.SmoothL1Loss()\n",
        "        q_loss = criterion(predicted_state_value, target_value.unsqueeze(1))\n",
        "        self.optimizer.zero_grad()\n",
        "        q_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "    def update_target_net(self):\n",
        "        if len(self.replay_memory.buffer) < self.params['batch_size']:\n",
        "            return\n",
        "        q_net_state_dict = self.q_net.state_dict()\n",
        "        target_net_state_dict = self.target_net.state_dict()\n",
        "        for key in q_net_state_dict:\n",
        "            target_net_state_dict[key] = self.params['tau']*q_net_state_dict[key] + (1 - self.params['tau'])*target_net_state_dict[key]\n",
        "        self.target_net.load_state_dict(target_net_state_dict)\n",
        "\n",
        "    def generate_video(self, max_frame=1000):\n",
        "        self.env = gym.make(self.params['env_name'], render_mode='rgb_array_list')\n",
        "        self.epsilon = 0.0\n",
        "        obs, _ = self.env.reset()\n",
        "        for _ in range(max_frame):\n",
        "            action = self.get_action(obs)\n",
        "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "        save_video(frames=self.env.render(), video_folder=self.params['env_name'][:-3], fps=self.env.metadata['render_fps'], step_starting_index=0, episode_index=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        experience = (state, action, reward, next_state, done)\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, n_samples):\n",
        "        batch = random.sample(self.buffer, n_samples)\n",
        "        batch = [e for e in batch if e is not None]\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        next_states = [\n",
        "                 np.zeros_like(state) if next_state is None else next_state\n",
        "                  for state, next_state in zip(states, next_states)\n",
        "                  ]\n",
        "\n",
        "        return (\n",
        "            torch.tensor(states, dtype=torch.float32),\n",
        "            torch.tensor(actions, dtype=torch.long),\n",
        "            torch.tensor(rewards, dtype=torch.float32),\n",
        "            torch.tensor(next_states, dtype=torch.float32),\n",
        "            torch.tensor(dones, dtype=torch.float32),\n",
        "        )\n",
        "\n",
        "\n",
        "class QNet(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim):\n",
        "        super(QNet, self).__init__()\n",
        "        self.ff_net = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, obs):\n",
        "        return self.ff_net(obs)"
      ],
      "metadata": {
        "id": "Txu5k46LWlvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_list = [\n",
        "    \n",
        " {\n",
        "    'env_name': 'CartPole-v1',\n",
        "    'rng_seed': 6369,\n",
        "    'n_episode': 35,\n",
        "    'rm_cap': 8192,\n",
        "    'batch_size': 128,\n",
        "    'hidden_dim': 128,\n",
        "    'init_epsilon': 0.9,\n",
        "    'min_epsilon': 0.5,\n",
        "    'epsilon_decay': 0.99,\n",
        "    'gamma': 0.99,\n",
        "    'tau': 0.5,\n",
        "    'lr': 3e-3,\n",
        "    'exp_name': 'CartPole_v1_t0'\n",
        "},\n",
        "    \n",
        " {\n",
        "    'env_name': 'CartPole-v1',\n",
        "    'rng_seed': 6369,\n",
        "    'n_episode': 35,\n",
        "    'rm_cap': 8192,\n",
        "    'batch_size': 128,\n",
        "    'hidden_dim': 128,\n",
        "    'init_epsilon': 0.9,\n",
        "    'min_epsilon': 0.5,\n",
        "    'epsilon_decay': 0.99,\n",
        "    'gamma': 0.99,\n",
        "    'tau': 0.05,\n",
        "    'lr': 3e-3,\n",
        "    'exp_name': 'CartPole_v1_t1'\n",
        "},\n",
        "     \n",
        " {\n",
        "    'env_name': 'CartPole-v1',\n",
        "    'rng_seed': 6369,\n",
        "    'n_episode': 35,\n",
        "    'rm_cap': 8192,\n",
        "    'batch_size': 128,\n",
        "    'hidden_dim': 128,\n",
        "    'init_epsilon': 0.9,\n",
        "    'min_epsilon': 0.5,\n",
        "    'epsilon_decay': 0.99,\n",
        "    'gamma': 0.99,\n",
        "    'tau': 0.005,\n",
        "    'lr': 3e-3,\n",
        "    'exp_name': 'CartPole_v1_t2'\n",
        "},\n",
        " {\n",
        "    'env_name': 'CartPole-v1',\n",
        "    'rng_seed': 6369,\n",
        "    'n_episode': 35,\n",
        "    'rm_cap': 8192,\n",
        "    'batch_size': 128,\n",
        "    'hidden_dim': 128,\n",
        "    'init_epsilon': 0.0,\n",
        "    'min_epsilon': 0.00,\n",
        "    'epsilon_decay': 0.99,\n",
        "    'gamma': 0.99,\n",
        "    'tau': 0.005,\n",
        "    'lr': 3e-3,\n",
        "    'exp_name': 'CartPole_v1_t3'\n",
        "},\n",
        "\n",
        "{\n",
        "    'env_name': 'CartPole-v1',\n",
        "    'rng_seed': 6369,\n",
        "    'n_episode': 35,\n",
        "    'rm_cap': 8192,\n",
        "    'batch_size': 128,\n",
        "    'hidden_dim': 128,\n",
        "    'init_epsilon': 0.1,\n",
        "    'min_epsilon': 0.05,\n",
        "    'epsilon_decay': 0.99,\n",
        "    'gamma': 0.99,\n",
        "    'tau': 0.005,\n",
        "    'lr': 3e-3,\n",
        "    'exp_name': 'CartPole_v1_t4'\n",
        "}\n",
        "]\n",
        "\n",
        "# Train agent with each set of parameters\n",
        "for params in params_list:\n",
        "    # Seed RNGs\n",
        "    seed_everything(params['rng_seed'])\n",
        "# Train agent\n",
        "    trainer = DQNTrainer(params)\n",
        "    trainer.run_training_loop()"
      ],
      "metadata": {
        "id": "BfA_uRsnWtPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the list of experiment names\n",
        "exp_names = ['CartPole_v1_t0', 'CartPole_v1_t1', 'CartPole_v1_t2', 'CartPole_v1_t3','CartPole_v1_t4']\n",
        "\n",
        "# Load the data for each experiment\n",
        "all_rewards = []\n",
        "for exp_name in exp_names:\n",
        "    file_name = exp_name + '.pkl'\n",
        "    with open(file_name, 'rb') as f:\n",
        "        rewards = pickle.load(f)\n",
        "    all_rewards.append(rewards)\n",
        "\n",
        "# Plot the data\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i in range(len(exp_names)):\n",
        "    sns.lineplot(data=all_rewards[i], label=exp_names[i])\n",
        "plt.xlabel('Rollout', fontsize=14)\n",
        "plt.ylabel('Reward', fontsize=14)\n",
        "plt.title('Learning Curves for CartPole with DQN', fontsize=16)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zdFeWa1GWxkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Define the list of experiment names\n",
        "exp_names = ['CartPole_v1_t0', 'CartPole_v1_t1', 'CartPole_v1_t2', 'CartPole_v1_t3']\n",
        "\n",
        "# Load the data for each experiment\n",
        "all_rewards = []\n",
        "for exp_name in exp_names:\n",
        "    file_name = exp_name + '.pkl'\n",
        "    with open(file_name, 'rb') as f:\n",
        "        rewards = pickle.load(f)\n",
        "    all_rewards.append(rewards)\n",
        "\n",
        "# Extract the rewards for exploration and exploitation\n",
        "exp_rewards = [exp[::2] for exp in all_rewards]\n",
        "exp_labels = [name + ' (Exploration)' for name in exp_names]\n",
        "exp_colors = sns.color_palette('husl', n_colors=len(exp_names))\n",
        "\n",
        "exp_rewards_flat = [val for sublist in exp_rewards for val in sublist]\n",
        "exp_labels_flat = [label for sublist in [[label] * len(exp) for label, exp in zip(exp_labels, exp_rewards)] for label in sublist]\n",
        "exp_colors_flat = [color for sublist in [[color] * len(exp) for color, exp in zip(exp_colors, exp_rewards)] for color in sublist]\n",
        "\n",
        "exp_data = {'Rollout': list(range(len(exp_rewards_flat))), 'Reward': exp_rewards_flat, 'Label': exp_labels_flat, 'Color': exp_colors_flat}\n",
        "\n",
        "exploit_rewards = [exp[1::2] for exp in all_rewards]\n",
        "exploit_labels = [name + ' (Exploitation)' for name in exp_names]\n",
        "exploit_colors = sns.color_palette('husl', n_colors=len(exp_names))\n",
        "\n",
        "exploit_rewards_flat = [val for sublist in exploit_rewards for val in sublist]\n",
        "exploit_labels_flat = [label for sublist in [[label] * len(exp) for label, exp in zip(exploit_labels, exploit_rewards)] for label in sublist]\n",
        "exploit_colors_flat = [color for sublist in [[color] * len(exp) for color, exp in zip(exploit_colors, exploit_rewards)] for color in sublist]\n",
        "\n",
        "exploit_data = {'Rollout': list(range(len(exploit_rewards_flat))), 'Reward': exploit_rewards_flat, 'Label': exploit_labels_flat, 'Color': exploit_colors_flat}\n",
        "\n",
        "# Combine the exploration and exploitation data\n",
        "data = pd.concat([pd.DataFrame(exp_data), pd.DataFrame(exploit_data)], axis=0)\n",
        "\n",
        "# Plot the data\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.lineplot(data=data, x='Rollout', y='Reward', hue='Label', palette=data['Color'].unique(), linewidth=2.5)\n",
        "plt.xlabel('Rollout', fontsize=14)\n",
        "plt.ylabel('Reward', fontsize=14)\n",
        "plt.title('Exploration vs Exploitation Learning Curves for CartPole with DQN', fontsize=16)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LbNOYs0XWy7Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}